<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Noah Broestl | AI Researcher and Practioner</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description"
        content="Noah Broestl – VP of Responsible AI at BCG and PhD candidate at Cambridge’s Centre for Human-Inspired AI.">
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link rel="icon" type="image/png" href="assets/img/favicon.png" />
</head>
<body>
  <!-- Header & Navigation -->
  <header class="site-header">
    <div class="container nav-container">
      <a href="#top" class="logo">Noah Broestl</a>
      <nav class="main-nav" id="mainNav">
        <a href="#about">About</a>
        <a href="#experience">Experience</a>
        <a href="#research">Selected Publications</a>
        <a href="#speaking">Speaking</a>
        <a href="#interests">Focus Areas</a>
        <a href="#contact">Contact</a>
      </nav>
      <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
        <span></span><span></span><span></span>
      </button>
    </div>
  </header>

  <main id="top">
    <!-- Hero -->
    <section class="hero">
      <div class="container hero-grid">
        <div class="hero-text">
          <p class="eyebrow">AI Researcher and Practioner</p>
          <h1>Hi, I’m Noah Broestl.</h1>
          <p class="hero-subtitle">
            Vice President of Responsible AI at Boston Consulting Group and
            PhD candidate at the University of Cambridge’s Centre for Human-Inspired AI (CHIA).
          </p>
          <p class="hero-body">
            I work with global organizations to design and govern AI systems that are aligned with
            human values, robust under real-world pressure, and accountable to the people they serve.
          </p>
          <div class="hero-actions">
            <a href="#contact" class="btn primary-btn">Get in touch</a>
            <a href="#research" class="btn ghost-btn">Explore my work</a>
          </div>
          <div class="hero-meta">
            <span>Brooklyn, NYC, US &nbsp;·&nbsp; Cambridge, UK</span>
          </div>
        </div>
        <div class="hero-image-card">
          <img src="assets/img/noah-broestl.jpg" alt="Portrait of Noah Broestl" />
          <div class="hero-badge">
            <span class="badge-label">Current roles</span>
            <ul>
              <li>Vice President, Responsible AI – BCG</li>
              <li>PhD Candidate – Cambridge University</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- About -->
    <section class="section" id="about">
      <div class="container section-grid">
        <div>
          <h2>About</h2>
          <p>
            My work sits at the intersection of AI, ethics, and governance. I’ve spent more than a
            decade working on high-stakes AI systems – from counter-abuse and health research at Google
            to generative AI safety and large-scale governance programs at BCG.
          </p>
          <p>
            I’m currently Vice President of Responsible AI at Boston Consulting Group,
            where I help organizations build responsible AI programs, design human oversight, and manage
            AI risk as a core part of business strategy. In parallel, I’m a PhD candidate in Human-Inspired
            AI at the University of Cambridge, studying how generative AI systems shape human decision-making
            and values.
          </p>
          <p>
            My path began in sociology, law, and practical ethics, and has taken me through the U.S. Air
            Force, Google, and now global consulting. That interdisciplinary background lets me connect
            technical realities, organizational constraints, and the ethical questions that matter.
          </p>
        </div>
        <div>
          <h3>Snapshot</h3>
          <ul class="key-facts">
            <li><strong>Current:</strong> Vice President of Responsible AI, BCG (2025–present)</li>
            <li><strong>Current:</strong> Partner &amp; AD of Responsible AI, BCG (2024–2025)</li>
            <li><strong>Current:</strong> PhD Candidate, Human-Inspired AI, University of Cambridge</li>
            <li><strong>Previously:</strong> 13 years at Google (2011–2024) in engineering, research &amp; counter-abuse</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- Experience -->
    <section class="section section-alt" id="experience">
      <div class="container">
        <div class="section-heading">
          <h2>Experience</h2>
          <p>Bridging AI research, product safety, and real-world governance.</p>
        </div>

        <div class="timeline">
          <!-- BCG -->
          <article class="timeline-item">
            <div class="timeline-meta">
              <h3>Boston Consulting Group (BCG)</h3>
              <span class="timeline-role">Partner &amp; Associate Director, Responsible AI</span>
              <span class="timeline-dates">2024 – Present · Brooklyn &amp; Global</span>
            </div>
            <div class="timeline-body">
              <p>
                I lead BCG’s work on Responsible AI, with a focus on generative AI governance, risk
                management, and human oversight. I help organizations design AI programs that are
                safe, compliant, and aligned with their values and strategies.
              </p>
              <ul>
                <li>Advise C-suite leaders on AI governance, organizational models, and operating frameworks.</li>
                <li>Design oversight mechanisms and escalation pathways for generative AI deployments.</li>
                <li>Co-author thought leadership on human oversight and AI failure preparedness.</li>
              </ul>
            </div>
          </article>

          <!-- Google -->
          <article class="timeline-item">
            <div class="timeline-meta">
              <h3>Google</h3>
              <span class="timeline-role">Senior Roles in AI Safety, Research &amp; Abuse</span>
              <span class="timeline-dates">2011 – 2024</span>
            </div>
            <div class="timeline-body">
              <p>
                Over 13 years at Google, I worked across Maps, Research, and generative AI, focused
                on building and evaluating systems that need to work safely at global scale.
              </p>
              <ul>
                <li><strong>Generative AI Safety:</strong> Led safety evaluations for Google’s first generative
                    AI product (Bard), including red-teaming and risk assessment of large language models.</li>
                <li><strong>Google Research (AI Health):</strong> Worked on Responsible AI for health, addressing
                    fairness, interpretability, and ethics in medical ML.</li>
                <li><strong>Google Maps:</strong> Led counter-abuse efforts to protect map integrity and users
                    from fraud and harm.</li>
                <li><strong>AI Residency Program:</strong> Managed the AI Residency across the U.S. East Coast
                    and EMEA, supporting early-career researchers and applied ML projects.</li>
              </ul>
            </div>
          </article>

          <!-- Air Force -->
          <article class="timeline-item">
            <div class="timeline-meta">
              <h3>United States Air Force</h3>
              <span class="timeline-role">Intelligence Analyst</span>
              <span class="timeline-dates">Pre-2011</span>
            </div>
            <div class="timeline-body">
              <p>
                I began my career as an intelligence analyst, working in mission-critical environments
                that demanded careful reasoning, oversight, and accountability—perspectives that still
                shape how I think about AI risk today.
              </p>
            </div>
          </article>
        </div>
      </div>
    </section>

    <!-- Education -->
    <section class="section" id="education">
      <div class="container">
        <div class="section-heading">
          <h2>Academic Background</h2>
          <p>Training in sociology, law, ethics, and human-inspired AI.</p>
        </div>

        <div class="cards-grid">
          <article class="card">
            <h3>PhD in Human-Inspired AI</h3>
            <p class="card-meta">
              University of Cambridge · Centre for Human-Inspired AI (CHIA) · 2024 – Present
            </p>
            <p>
              Researching how generative AI systems shape human decision-making, values, and
              behavior, and how governance can respond to those affordances. Co-supervised by
              leading scholars in AI ethics and governance.
            </p>
          </article>

          <article class="card">
            <h3>MSt in Practical Ethics (Distinction)</h3>
            <p class="card-meta">
              University of Oxford · 2024
            </p>
            <p>
              Master’s work in practical ethics, with a thesis on what kinds of explanations AI systems
              need to provide to build trust and protect personal autonomy.
            </p>
          </article>

          <article class="card">
            <h3>B.A. Sociology &amp; B.A. Liberal Arts</h3>
            <p class="card-meta">
              Colorado State University
            </p>
            <p>
              Dual bachelor’s degrees in Sociology (with a Certificate in Diversity in Law) and Liberal
              Arts (History minor), grounding my work in social science and the law’s approach to
              fairness and inclusion.
            </p>
          </article>
        </div>
      </div>
    </section>

    <!-- Selected Publications -->
    <section class="section section-alt" id="research">
      <div class="container">
        <div class="section-heading">
          <h2>Selected Publications</h2>
          <p>Selected work at the intersection of AI governance, ethics, and practice.</p>
        </div>

        <div class="cards-grid">
          <article class="card">
            <h3>Documenting Deployment with Fabric</h3>
            <p class="card-meta">
              AAAI/ACM Conference on AI, Ethics &amp; Society (AIES) · 2025
            </p>
            <p>
              Co-author on a paper introducing “Fabric”, a public repository of real-world AI use cases
              and governance practices, built from practitioner interviews across organizations.
            </p>
            <a href="https://arxiv.org/abs/2508.14119" target="_blank" rel="noopener" class="card-link">
              Read on arXiv
            </a>
          </article>

          <article class="card">
            <h3>You Won’t Get GenAI Right If You Get Human Oversight Wrong</h3>
            <p class="card-meta">
              BCG Publication · 2025
            </p>
            <p>
              Co-authored article on why simply putting “a human in the loop” isn’t enough, and how to
              design effective human oversight for generative AI systems in practice.
            </p>
            <a href="https://www.bcg.com/publications/2025/wont-get-gen-ai-right-if-human-oversight-wrong"
               target="_blank" rel="noopener" class="card-link">
              Read the article
            </a>
          </article>

          <article class="card">
            <h3>GenAI Will Fail. Prepare for It.</h3>
            <p class="card-meta">
              BCG Publication · 2024
            </p>
            <p>
              Lead author on a piece describing how organizations can anticipate and respond to
              generative AI failures, from monitoring and escalation to incident response.
            </p>
            <a href="https://www.bcg.com/publications/2024/genai-will-fail-how-to-prepare"
               target="_blank" rel="noopener" class="card-link">
              Read the article
            </a>
          </article>

          <article class="card">
            <h3>From Imitation to Insight</h3>
            <p class="card-meta">
              King’s Entrepreneurship Lab Blog · 2024
            </p>
            <p>
              Co-authored essay arguing for “process-based” AI evaluation—assessing how systems reach
              conclusions, not just whether their outputs resemble human answers.
            </p>
            <a href="https://www.kingselab.org/blog/bernardo-villegas-moreno-drew-calcagno-and-noah-broestl"
               target="_blank" rel="noopener" class="card-link">
              Read the essay
            </a>
          </article>
        </div>
      </div>
    </section>

    <!-- Speaking & Media -->
    <section class="section" id="speaking">
      <div class="container">
        <div class="section-heading">
          <h2>Speaking &amp; Media</h2>
          <p>Engagements on AI governance, ethics, and leadership.</p>
        </div>

        <div class="cards-grid">
          <article class="card">
            <h3>Princeton Energy Conference</h3>
            <p class="card-meta">
              Featured Speaker · 2024
            </p>
            <p>
              Spoke on building robust and responsible AI programs in the context of energy and
              climate, connecting AI risk management with critical infrastructure.
            </p>
          </article>

          <article class="card">
            <h3>Envision 2025 – AI Policy &amp; Ethics</h3>
            <p class="card-meta">
              Panelist · Princeton University
            </p>
            <p>
              Joined academics, policymakers, and industry leaders to discuss AI governance,
              regulatory trends, and practical approaches to aligning AI with societal needs.
            </p>
          </article>

          <article class="card">
            <h3>CXO Bytes – Navigating AI Risks</h3>
            <p class="card-meta">
              Podcast Guest · Green Software Foundation · 2025
            </p>
            <p>
              Discussed how leaders can innovate with generative AI while managing risks, with a
              special emphasis on sustainability and organizational governance.
            </p>
          </article>

          <article class="card">
            <h3>Imagine This… “If AGI’s a No-Show”</h3>
            <p class="card-meta">
              BCG podcast / video series
            </p>
            <p>
              Explored futures in which AI remains powerful but not fully “general,” and what this
              means for alignment, governance, and long-term AI strategy.
            </p>
          </article>

          <article class="card">
            <h3>Media &amp; Commentary</h3>
            <p class="card-meta">
              Fortune, academic blogs, university programs
            </p>
            <p>
              Quoted on the rise of Chief AI Officers in higher education and industry, and involved in
              Cambridge leadership programs that help executives develop responsible AI strategies.
            </p>
          </article>
        </div>
      </div>
    </section>

    <!-- Interests / Focus Areas -->
    <section class="section section-alt" id="interests">
      <div class="container">
        <div class="section-heading">
          <h2>Focus Areas</h2>
          <p>What I’m currently working on and thinking about.</p>
        </div>

        <div class="pill-grid">
          <article class="pill">
            <h3>Responsible AI &amp; Ethics</h3>
            <p>
              Integrating ethical principles—fairness, accountability, respect for autonomy—into the
              design and deployment of AI systems from day one.
            </p>
          </article>

          <article class="pill">
            <h3>AI Governance &amp; Risk</h3>
            <p>
              Building governance frameworks, oversight mechanisms, and incident response plans that
              help organizations use AI safely and confidently.
            </p>
          </article>

          <article class="pill">
            <h3>Transparency &amp; Explainability</h3>
            <p>
              Developing and advocating for AI systems that can surface evidence for and against their
              outputs, and explain themselves in ways that support human judgment.
            </p>
          </article>

          <article class="pill">
            <h3>Fairness &amp; Inclusion</h3>
            <p>
              Addressing bias in data and models, and considering how AI systems affect different
              communities and their opportunities.
            </p>
          </article>

          <article class="pill">
            <h3>AI Alignment &amp; Safety</h3>
            <p>
              Evaluating generative models in adversarial settings, strengthening safeguards, and
              keeping human judgment central even as AI systems grow more capable.
            </p>
          </article>

          <article class="pill">
            <h3>AI &amp; Society</h3>
            <p>
              Exploring how AI interacts with institutions, regulation, and long-term societal goals,
              including sustainability and climate.
            </p>
          </article>
        </div>
      </div>
    </section>

    <!-- Contact -->
    <section class="section" id="contact">
      <div class="container section-grid">
        <div>
          <h2>Contact</h2>
          <p>
            For speaking, collaboration, or advisory requests related to Responsible AI, AI governance,
            or AI leadership programs, the best way to reach me is via email.
          </p>
          <ul class="contact-list">
            <li><strong>Email:</strong> <a href="mailto:nbroestl@gmail.com">nbroestl@gmail.com</a></li>
          </ul>
        </div>
        <div>
          <h3>Speaking &amp; collaboration</h3>
          <p>
            I regularly speak with executives, policymakers, and researchers about:
          </p>
          <ul class="key-facts">
            <li>Designing responsible AI programs and governance structures</li>
            <li>Human oversight for generative AI</li>
            <li>AI leadership in universities and enterprises</li>
            <li>AI, climate, and sustainable software</li>
          </ul>
        </div>
      </div>
    </section>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container footer-grid">
      <p>&copy; <span id="year"></span> Noah Broestl. All rights reserved.</p>
      <p class="footer-links">
        <a href="#top">Back to top</a>
      </p>
    </div>
  </footer>

  <button id="backToTop" class="back-to-top" aria-label="Back to top">↑</button>

  <script src="assets/js/main.js"></script>
</body>
</html>
